/*! For license information please see sample.bundle.js.LICENSE.txt */
(()=>{var __webpack_modules__={"./src/sample.js":()=>{eval('// PPG Demo by Andy Kong\r\n// History tracking for the intensity of the face box\r\n\r\n// Set up variables for tracking the average intensity\r\n// The desktop\'s camera goes at around 10Hz, and we want ~5 seconds of history\r\nvar maxHistLen = 64;\r\nvar bloodHist = Array(maxHistLen).fill(0);\r\nvar timingHist = Array(maxHistLen).fill(0);\r\nvar last = performance.now();\r\n\r\n// A one-liners to help us track the history\r\nvar average = (array) => array.reduce((a, b) => a + b) / array.length;\r\n\r\n// Face Mesh Demo by Andy Kong\r\n// Base Javascript for setting up a camera-streaming HTML webpage.\r\n\r\nasync function setupCamera() {\r\n  // Find the <video> element in the webpage,\r\n  // then use the mediaDevices API to request a camera from the user\r\n  video = document.getElementById("video");\r\n  const stream = await navigator.mediaDevices.getUserMedia({\r\n    audio: false,\r\n    video: {\r\n      facingMode: "user",\r\n      width: { ideal: 1920 },\r\n      height: { ideal: 1080 },\r\n    },\r\n  });\r\n  // Assign our camera to the HTML\'s video element\r\n  video.srcObject = stream;\r\n\r\n  return new Promise((resolve) => {\r\n    video.onloadedmetadata = () => {\r\n      resolve(video);\r\n    };\r\n  });\r\n}\r\n\r\nasync function drawVideo() {\r\n  ctx.drawImage(video, 0, 0);\r\n  for (face of curFaces) {\r\n    drawFace(face);\r\n  }\r\n  requestAnimationFrame(drawVideo);\r\n}\r\n\r\n// Draws the current eyes onto the canvas, directly from video streams\r\nasync function drawFaces() {\r\n  ctx.strokeStyle = "cyan";\r\n  ctx.lineWidth = 2;\r\n  for (face of curFaces) {\r\n    if (face.faceInViewConfidence > 0.9) {\r\n      let mesh = face.scaledMesh;\r\n\r\n      // Get the facial region of interest\'s bounds\r\n      boxLeft = mesh[117][0];\r\n      boxTop = mesh[117][1];\r\n      boxWidth = mesh[346][0] - boxLeft;\r\n      boxHeight = mesh[164][1] - boxTop;\r\n\r\n      // Draw the box a bit larger for debugging purposes\r\n      ctx.beginPath();\r\n      const boxsize = 4;\r\n      ctx.rect(\r\n        boxLeft - boxsize,\r\n        boxTop - boxsize,\r\n        boxWidth + boxsize * 2,\r\n        boxHeight + boxsize * 2\r\n      );\r\n      ctx.stroke();\r\n      // Get the image data from that region\r\n      let bloodRegion = ctx.getImageData(boxLeft, boxTop, boxWidth, boxHeight);\r\n\r\n      // Get the area into Tensorflow, then split and average it\r\n      videoDataSum = bloodRegion.data.reduce((a, b) => a + b, 0);\r\n      videoDataSum -= boxWidth * boxHeight * 255; // remove alpha channel\r\n      avgIntensity = videoDataSum / (boxWidth * boxHeight * 3);\r\n\r\n      // Track FPS of this loop as well\r\n      timingHist.push(1 / ((performance.now() - last) * 0.001));\r\n      last = performance.now();\r\n\r\n      // Append intensity and FPS to an array and shift out the first element if the array gets too long\r\n      bloodHist.push(bloodHist[maxHistLen - 1] * 0.8 + 0.2 * avgIntensity);\r\n      if (bloodHist.length > maxHistLen) {\r\n        bloodHist.shift();\r\n        timingHist.shift();\r\n\r\n        fftData = await calcFFT(bloodHist);\r\n        updateChart(timingHist, fftData);\r\n        updateChart2(bloodHist);\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n// Set up variables to draw on the canvas\r\nvar canvas;\r\nvar ctx;\r\nasync function main() {\r\n  // Set up front-facing camera\r\n  await setupCamera();\r\n  videoWidth = video.videoWidth;\r\n  videoHeight = video.videoHeight;\r\n  video.play();\r\n\r\n  // Set up the HTML Canvas to draw the video feed onto\r\n  canvas = document.getElementById("facecanvas");\r\n  canvas.width = videoWidth;\r\n  canvas.height = videoHeight;\r\n  ctx = canvas.getContext("2d");\r\n\r\n  // Start the video->canvas drawing loop\r\n  drawVideo();\r\n  fmesh = await facemesh.load({ detectionConfidence: 0.9, maxFaces: 3 });\r\n}\r\n\r\nvar curFaces = [];\r\nasync function renderPrediction() {\r\n  // Call face Mesh on our video stream\r\n  const facepred = await fmesh.estimateFaces(video);\r\n\r\n  // If we find a face, export it to a global variable so we can access it elsewhere\r\n  if (facepred.length > 0) {\r\n    curFaces = facepred;\r\n  }\r\n  // Call itself again\r\n  requestAnimationFrame(renderPrediction);\r\n}\r\n\n\n//# sourceURL=webpack://facehealthmonitoring/./src/sample.js?')}},__webpack_exports__={};__webpack_modules__["./src/sample.js"]()})();